{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats import stattools\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RiskLabAI.controller import Controller\n",
    "# initialize controller\n",
    "controller = Controller()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "from optuna.exceptions import TrialPruned\n",
    "from optuna.samplers import QMCSampler, CmaEsSampler\n",
    "import torch\n",
    "print(torch.backends.mps.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>symbol</th>\n",
       "      <th>ts_ms</th>\n",
       "      <th>iso_utc</th>\n",
       "      <th>ohlc_ts_open</th>\n",
       "      <th>ohlc_open</th>\n",
       "      <th>ohlc_high</th>\n",
       "      <th>ohlc_low</th>\n",
       "      <th>ohlc_close</th>\n",
       "      <th>ohlc_volume</th>\n",
       "      <th>ohlc_ts_close</th>\n",
       "      <th>...</th>\n",
       "      <th>tr_volume_base</th>\n",
       "      <th>tr_volume_quote</th>\n",
       "      <th>tr_vwap</th>\n",
       "      <th>tr_buy_sell_imbalance</th>\n",
       "      <th>spot_price</th>\n",
       "      <th>perp_mark_price</th>\n",
       "      <th>basis_abs</th>\n",
       "      <th>basis_pct</th>\n",
       "      <th>funding_rate</th>\n",
       "      <th>next_funding_time_ms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>39162</th>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>1757821722142</td>\n",
       "      <td>2025-09-14T03:48:42.142655+00:00</td>\n",
       "      <td>1757821680000</td>\n",
       "      <td>115741.99</td>\n",
       "      <td>115742.00</td>\n",
       "      <td>115729.25</td>\n",
       "      <td>115729.26</td>\n",
       "      <td>3.68421</td>\n",
       "      <td>1757821739999</td>\n",
       "      <td>...</td>\n",
       "      <td>12.22828</td>\n",
       "      <td>1.415327e+06</td>\n",
       "      <td>115742.129388</td>\n",
       "      <td>-0.359820</td>\n",
       "      <td>115729.25</td>\n",
       "      <td>115674.594917</td>\n",
       "      <td>-54.655083</td>\n",
       "      <td>-0.000472</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>1757836800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39163</th>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>1757821782162</td>\n",
       "      <td>2025-09-14T03:49:42.162661+00:00</td>\n",
       "      <td>1757821740000</td>\n",
       "      <td>115729.25</td>\n",
       "      <td>115746.27</td>\n",
       "      <td>115729.25</td>\n",
       "      <td>115746.27</td>\n",
       "      <td>2.35966</td>\n",
       "      <td>1757821799999</td>\n",
       "      <td>...</td>\n",
       "      <td>7.30407</td>\n",
       "      <td>8.453633e+05</td>\n",
       "      <td>115738.666289</td>\n",
       "      <td>0.406802</td>\n",
       "      <td>115746.27</td>\n",
       "      <td>115704.300000</td>\n",
       "      <td>-41.970000</td>\n",
       "      <td>-0.000363</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>1757836800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39164</th>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>1757821842222</td>\n",
       "      <td>2025-09-14T03:50:42.222298+00:00</td>\n",
       "      <td>1757821800000</td>\n",
       "      <td>115746.27</td>\n",
       "      <td>115746.27</td>\n",
       "      <td>115746.26</td>\n",
       "      <td>115746.26</td>\n",
       "      <td>1.97252</td>\n",
       "      <td>1757821859999</td>\n",
       "      <td>...</td>\n",
       "      <td>5.76467</td>\n",
       "      <td>6.672202e+05</td>\n",
       "      <td>115743.000526</td>\n",
       "      <td>0.167002</td>\n",
       "      <td>115746.26</td>\n",
       "      <td>115705.200271</td>\n",
       "      <td>-41.059729</td>\n",
       "      <td>-0.000355</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>1757836800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39165</th>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>1757821902272</td>\n",
       "      <td>2025-09-14T03:51:42.272630+00:00</td>\n",
       "      <td>1757821860000</td>\n",
       "      <td>115742.13</td>\n",
       "      <td>115742.13</td>\n",
       "      <td>115740.04</td>\n",
       "      <td>115740.04</td>\n",
       "      <td>1.64508</td>\n",
       "      <td>1757821919999</td>\n",
       "      <td>...</td>\n",
       "      <td>7.64589</td>\n",
       "      <td>8.849185e+05</td>\n",
       "      <td>115737.800656</td>\n",
       "      <td>-0.275776</td>\n",
       "      <td>115740.04</td>\n",
       "      <td>115688.728758</td>\n",
       "      <td>-51.311242</td>\n",
       "      <td>-0.000443</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>1757836800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39166</th>\n",
       "      <td>BTCUSDT</td>\n",
       "      <td>1757821962322</td>\n",
       "      <td>2025-09-14T03:52:42.322623+00:00</td>\n",
       "      <td>1757821920000</td>\n",
       "      <td>115740.05</td>\n",
       "      <td>115740.05</td>\n",
       "      <td>115740.04</td>\n",
       "      <td>115740.04</td>\n",
       "      <td>1.43726</td>\n",
       "      <td>1757821979999</td>\n",
       "      <td>...</td>\n",
       "      <td>8.60731</td>\n",
       "      <td>9.962008e+05</td>\n",
       "      <td>115738.920133</td>\n",
       "      <td>0.084968</td>\n",
       "      <td>115740.04</td>\n",
       "      <td>115688.258712</td>\n",
       "      <td>-51.781287</td>\n",
       "      <td>-0.000447</td>\n",
       "      <td>0.000082</td>\n",
       "      <td>1757836800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        symbol          ts_ms                           iso_utc  \\\n",
       "39162  BTCUSDT  1757821722142  2025-09-14T03:48:42.142655+00:00   \n",
       "39163  BTCUSDT  1757821782162  2025-09-14T03:49:42.162661+00:00   \n",
       "39164  BTCUSDT  1757821842222  2025-09-14T03:50:42.222298+00:00   \n",
       "39165  BTCUSDT  1757821902272  2025-09-14T03:51:42.272630+00:00   \n",
       "39166  BTCUSDT  1757821962322  2025-09-14T03:52:42.322623+00:00   \n",
       "\n",
       "        ohlc_ts_open  ohlc_open  ohlc_high   ohlc_low  ohlc_close  \\\n",
       "39162  1757821680000  115741.99  115742.00  115729.25   115729.26   \n",
       "39163  1757821740000  115729.25  115746.27  115729.25   115746.27   \n",
       "39164  1757821800000  115746.27  115746.27  115746.26   115746.26   \n",
       "39165  1757821860000  115742.13  115742.13  115740.04   115740.04   \n",
       "39166  1757821920000  115740.05  115740.05  115740.04   115740.04   \n",
       "\n",
       "       ohlc_volume  ohlc_ts_close  ...  tr_volume_base  tr_volume_quote  \\\n",
       "39162      3.68421  1757821739999  ...        12.22828     1.415327e+06   \n",
       "39163      2.35966  1757821799999  ...         7.30407     8.453633e+05   \n",
       "39164      1.97252  1757821859999  ...         5.76467     6.672202e+05   \n",
       "39165      1.64508  1757821919999  ...         7.64589     8.849185e+05   \n",
       "39166      1.43726  1757821979999  ...         8.60731     9.962008e+05   \n",
       "\n",
       "             tr_vwap  tr_buy_sell_imbalance  spot_price  perp_mark_price  \\\n",
       "39162  115742.129388              -0.359820   115729.25    115674.594917   \n",
       "39163  115738.666289               0.406802   115746.27    115704.300000   \n",
       "39164  115743.000526               0.167002   115746.26    115705.200271   \n",
       "39165  115737.800656              -0.275776   115740.04    115688.728758   \n",
       "39166  115738.920133               0.084968   115740.04    115688.258712   \n",
       "\n",
       "       basis_abs  basis_pct  funding_rate  next_funding_time_ms  \n",
       "39162 -54.655083  -0.000472      0.000082         1757836800000  \n",
       "39163 -41.970000  -0.000363      0.000082         1757836800000  \n",
       "39164 -41.059729  -0.000355      0.000082         1757836800000  \n",
       "39165 -51.311242  -0.000443      0.000082         1757836800000  \n",
       "39166 -51.781287  -0.000447      0.000082         1757836800000  \n",
       "\n",
       "[5 rows x 37 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Asset under study\n",
    "ticker = 'BTCUSDT'\n",
    "\n",
    "# define dataset\n",
    "dataset = ds.dataset(\n",
    "    \"/Users/bobet/Documents/Code Repository/Trading-Systems/_datasets\",\n",
    "    format=\"parquet\")\n",
    "\n",
    "# push filter into Arrow scan (faster, uses partition pruning if possible)\n",
    "table = dataset.to_table(filter=ds.field(\"symbol\") == ticker)\n",
    "\n",
    "# convert to pandasssss\n",
    "df = table.to_pandas()\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['symbol', 'ts_ms', 'iso_utc', 'ohlc_ts_open', 'ohlc_open', 'ohlc_high',\n",
       "       'ohlc_low', 'ohlc_close', 'ohlc_volume', 'ohlc_ts_close', 'ohlc_trades',\n",
       "       'ohlc_taker_base', 'ohlc_taker_quote', 'l1_bid', 'l1_ask', 'l1_mid',\n",
       "       'l1_spread', 'l1_bid_qty', 'l1_ask_qty', 'l1_imbalance', 'l2_bid_depth',\n",
       "       'l2_ask_depth', 'l2_depth_asymmetry', 'l2_bid_vwap', 'l2_ask_vwap',\n",
       "       'l2_bid_slope', 'l2_ask_slope', 'tr_volume_base', 'tr_volume_quote',\n",
       "       'tr_vwap', 'tr_buy_sell_imbalance', 'spot_price', 'perp_mark_price',\n",
       "       'basis_abs', 'basis_pct', 'funding_rate', 'next_funding_time_ms'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#features\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 39167 entries, 0 to 39166\n",
      "Data columns (total 37 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   symbol                 39167 non-null  object \n",
      " 1   ts_ms                  39167 non-null  int64  \n",
      " 2   iso_utc                39167 non-null  object \n",
      " 3   ohlc_ts_open           39167 non-null  int64  \n",
      " 4   ohlc_open              39167 non-null  float64\n",
      " 5   ohlc_high              39167 non-null  float64\n",
      " 6   ohlc_low               39167 non-null  float64\n",
      " 7   ohlc_close             39167 non-null  float64\n",
      " 8   ohlc_volume            39167 non-null  float64\n",
      " 9   ohlc_ts_close          39167 non-null  int64  \n",
      " 10  ohlc_trades            39167 non-null  int64  \n",
      " 11  ohlc_taker_base        39167 non-null  float64\n",
      " 12  ohlc_taker_quote       39167 non-null  float64\n",
      " 13  l1_bid                 39167 non-null  float64\n",
      " 14  l1_ask                 39167 non-null  float64\n",
      " 15  l1_mid                 39167 non-null  float64\n",
      " 16  l1_spread              39167 non-null  float64\n",
      " 17  l1_bid_qty             39167 non-null  float64\n",
      " 18  l1_ask_qty             39167 non-null  float64\n",
      " 19  l1_imbalance           39167 non-null  float64\n",
      " 20  l2_bid_depth           39167 non-null  float64\n",
      " 21  l2_ask_depth           39167 non-null  float64\n",
      " 22  l2_depth_asymmetry     39167 non-null  float64\n",
      " 23  l2_bid_vwap            39167 non-null  float64\n",
      " 24  l2_ask_vwap            39167 non-null  float64\n",
      " 25  l2_bid_slope           39167 non-null  float64\n",
      " 26  l2_ask_slope           39167 non-null  float64\n",
      " 27  tr_volume_base         39167 non-null  float64\n",
      " 28  tr_volume_quote        39167 non-null  float64\n",
      " 29  tr_vwap                39167 non-null  float64\n",
      " 30  tr_buy_sell_imbalance  39167 non-null  float64\n",
      " 31  spot_price             39167 non-null  float64\n",
      " 32  perp_mark_price        39167 non-null  float64\n",
      " 33  basis_abs              39167 non-null  float64\n",
      " 34  basis_pct              39167 non-null  float64\n",
      " 35  funding_rate           39167 non-null  float64\n",
      " 36  next_funding_time_ms   39167 non-null  int64  \n",
      "dtypes: float64(30), int64(5), object(2)\n",
      "memory usage: 11.1+ MB\n"
     ]
    }
   ],
   "source": [
    "sample_size = df.count()[0]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Features\n",
    "\n",
    "### General\n",
    "- **symbol**: Trading pair identifier (e.g., BTCUSDT).  \n",
    "- **ts_ms**: Data timestamp in milliseconds (epoch time).  \n",
    "- **iso_utc**: Data timestamp in human-readable UTC format.  \n",
    "\n",
    "### OHLC Data (Candlestick)\n",
    "- **ohlc_ts_open**: Opening timestamp for the candlestick period.  \n",
    "- **ohlc_open**: Opening price of the candlestick.  \n",
    "- **ohlc_high**: Highest price within the candlestick.  \n",
    "- **ohlc_low**: Lowest price within the candlestick.  \n",
    "- **ohlc_close**: Closing price of the candlestick.  \n",
    "- **ohlc_volume**: Trading volume during the candlestick (base asset units).  \n",
    "- **ohlc_ts_close**: Closing timestamp for the candlestick period.  \n",
    "- **ohlc_trades**: Number of trades in the candlestick.  \n",
    "- **ohlc_taker_base**: Base asset volume traded by takers (aggressors).  \n",
    "- **ohlc_taker_quote**: Quote asset volume traded by takers.  \n",
    "\n",
    "### Level 1 Order Book (Top of Book)\n",
    "- **l1_bid**: Best bid price (highest buy order).  \n",
    "- **l1_ask**: Best ask price (lowest sell order).  \n",
    "- **l1_mid**: Midpoint price between bid and ask.  \n",
    "- **l1_spread**: Difference between best ask and bid (ask - bid).  \n",
    "- **l1_bid_qty**: Quantity available at best bid.  \n",
    "- **l1_ask_qty**: Quantity available at best ask.  \n",
    "- **l1_imbalance**: Order book imbalance at Level 1 = (bid_qty ‚Äì ask_qty) / (bid_qty + ask_qty).  \n",
    "\n",
    "### Level 2 Order Book (Depth of Market)\n",
    "- **l2_bid_depth**: Total buy-side liquidity across multiple bid levels.  \n",
    "- **l2_ask_depth**: Total sell-side liquidity across multiple ask levels.  \n",
    "- **l2_depth_asymmetry**: Relative difference between bid and ask depth.  \n",
    "- **l2_bid_vwap**: Volume-weighted average bid price across order book levels.  \n",
    "- **l2_ask_vwap**: Volume-weighted average ask price across order book levels.  \n",
    "- **l2_bid_slope**: Measure of how steeply bid prices rise with quantity (liquidity gradient).  \n",
    "- **l2_ask_slope**: Measure of how steeply ask prices rise with quantity.  \n",
    "\n",
    "### Trade Data\n",
    "- **tr_volume_base**: Total traded volume in base asset.  \n",
    "- **tr_volume_quote**: Total traded volume in quote asset.  \n",
    "- **tr_vwap**: Trade volume-weighted average price.  \n",
    "- **tr_buy_sell_imbalance**: Difference between buy-initiated and sell-initiated trade volumes.  \n",
    "\n",
    "### Derived Prices\n",
    "- **spot_price**: Current spot market price.  \n",
    "- **perp_mark_price**: Mark price used in perpetual futures to avoid manipulation.  \n",
    "- **basis_abs**: Absolute difference between perpetual mark price and spot price.  \n",
    "- **basis_pct**: Percentage difference between perpetual mark price and spot price.  \n",
    "- **funding_rate**: Periodic payment rate between long and short positions in perpetual contracts.  \n",
    "- **next_funding_time_ms**: Timestamp (ms) of the next funding event.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data hygiene & storage\n",
    "\n",
    "‚úî Why: storage efficiency + ordering. A 10GB dataset may shrink to ~3-4GB when optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure correct dtypes (saves memory on 10GB dataset)\n",
    "dtype_map = {\n",
    "    \"symbol\": \"category\",\n",
    "    \"ohlc_open\": \"float32\", \"ohlc_high\": \"float32\", \"ohlc_low\": \"float32\", \"ohlc_close\": \"float32\",\n",
    "    \"ohlc_volume\": \"float32\", \"ohlc_trades\": \"int32\",\n",
    "    \"ohlc_taker_base\": \"float32\", \"ohlc_taker_quote\": \"float32\",\n",
    "    \"l1_bid\": \"float32\", \"l1_ask\": \"float32\", \"l1_mid\": \"float32\", \"l1_spread\": \"float32\",\n",
    "    \"l1_bid_qty\": \"float32\", \"l1_ask_qty\": \"float32\", \"l1_imbalance\": \"float32\",\n",
    "    \"l2_bid_depth\": \"float32\", \"l2_ask_depth\": \"float32\", \"l2_depth_asymmetry\": \"float32\",\n",
    "    \"l2_bid_vwap\": \"float32\", \"l2_ask_vwap\": \"float32\",\n",
    "    \"l2_bid_slope\": \"float32\", \"l2_ask_slope\": \"float32\",\n",
    "    \"tr_volume_base\": \"float32\", \"tr_volume_quote\": \"float32\", \"tr_vwap\": \"float32\",\n",
    "    \"tr_buy_sell_imbalance\": \"float32\",\n",
    "    \"spot_price\": \"float32\", \"perp_mark_price\": \"float32\",\n",
    "    \"basis_abs\": \"float32\", \"basis_pct\": \"float32\", \"funding_rate\": \"float32\"\n",
    "}\n",
    "\n",
    "df = df.astype(dtype_map)\n",
    "\n",
    "# Make sure timestamp is datetime\n",
    "df[\"iso_utc\"] = pd.to_datetime(df[\"iso_utc\"])\n",
    "df = df.set_index(\"iso_utc\").sort_index()\n",
    "\n",
    "\n",
    "#Intergrity Check\n",
    "# Drop duplicates, check ordering\n",
    "df = df[~df.index.duplicated(keep=\"first\")].sort_index()\n",
    "\n",
    "# Sanity checks for OHLC\n",
    "mask = (\n",
    "    (df[\"ohlc_low\"] <= df[\"ohlc_open\"]) &\n",
    "    (df[\"ohlc_low\"] <= df[\"ohlc_close\"]) &\n",
    "    (df[\"ohlc_high\"] >= df[\"ohlc_open\"]) &\n",
    "    (df[\"ohlc_high\"] >= df[\"ohlc_close\"])\n",
    ")\n",
    "df = df[mask]\n",
    "\n",
    "# Check non-negative volumes\n",
    "df = df[df[\"ohlc_volume\"] >= 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert DataFrame to RiskLA AI Input Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symbol</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BTCUSDT</th>\n",
       "      <td>2025-08-17 15:45:47.575950+00:00</td>\n",
       "      <td>118251.351562</td>\n",
       "      <td>0.75580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTCUSDT</th>\n",
       "      <td>2025-08-17 15:46:47.606265+00:00</td>\n",
       "      <td>118234.531250</td>\n",
       "      <td>5.14589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTCUSDT</th>\n",
       "      <td>2025-08-17 15:47:47.643040+00:00</td>\n",
       "      <td>118234.531250</td>\n",
       "      <td>2.35213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTCUSDT</th>\n",
       "      <td>2025-08-17 15:48:47.662644+00:00</td>\n",
       "      <td>118234.523438</td>\n",
       "      <td>6.03409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BTCUSDT</th>\n",
       "      <td>2025-08-17 15:49:47.702649+00:00</td>\n",
       "      <td>118234.523438</td>\n",
       "      <td>1.28289</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    date          price   volume\n",
       "symbol                                                          \n",
       "BTCUSDT 2025-08-17 15:45:47.575950+00:00  118251.351562  0.75580\n",
       "BTCUSDT 2025-08-17 15:46:47.606265+00:00  118234.531250  5.14589\n",
       "BTCUSDT 2025-08-17 15:47:47.643040+00:00  118234.531250  2.35213\n",
       "BTCUSDT 2025-08-17 15:48:47.662644+00:00  118234.523438  6.03409\n",
       "BTCUSDT 2025-08-17 15:49:47.702649+00:00  118234.523438  1.28289"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_riskAI = df.copy()\n",
    "df_riskAI = df_riskAI.loc[:, ['symbol', 'ohlc_close', 'ohlc_volume']]\n",
    "#rename column\n",
    "df_riskAI.reset_index(inplace=True) \n",
    "df_riskAI.set_index('symbol', inplace=True)\n",
    "df_riskAI.columns = ['date', 'price', 'volume']\n",
    "df_riskAI.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Existing Parameter Resampling Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ticker': 'BTCUSDT',\n",
       "  'time_frame': '1m',\n",
       "  'Sampling_Scheme': 'expected_tick_imbalance_bars',\n",
       "  'n_trials': 50,\n",
       "  'n_samples': 39167,\n",
       "  'batch_size': 30275161,\n",
       "  'Shapiro-Wilk_statistic': 0,\n",
       "  'Shapiro-Wilk_pvalue': 0,\n",
       "  'window_size_for_expected_n_ticks_estimation': 5,\n",
       "  'window_size_for_expected_imbalance_estimation': 2000,\n",
       "  'initial_estimate_of_expected_n_ticks_in_bar': 50},\n",
       " {'ticker': 'BTCUSDT',\n",
       "  'time_frame': '1m',\n",
       "  'Sampling_Scheme': 'expected_volume_imbalance_bars',\n",
       "  'n_trials': 50,\n",
       "  'n_samples': 39167,\n",
       "  'batch_size': 30275161,\n",
       "  'Shapiro-Wilk_statistic': 0,\n",
       "  'Shapiro-Wilk_pvalue': 0,\n",
       "  'window_size_for_expected_n_ticks_estimation': 5,\n",
       "  'window_size_for_expected_imbalance_estimation': 2000,\n",
       "  'initial_estimate_of_expected_n_ticks_in_bar': 50},\n",
       " {'ticker': 'BTCUSDT',\n",
       "  'time_frame': '1m',\n",
       "  'Sampling_Scheme': 'expected_dollar_imbalance_bars',\n",
       "  'n_trials': 50,\n",
       "  'n_samples': 39167,\n",
       "  'batch_size': 30275161,\n",
       "  'Shapiro-Wilk_statistic': 0,\n",
       "  'Shapiro-Wilk_pvalue': 0,\n",
       "  'window_size_for_expected_n_ticks_estimation': 5,\n",
       "  'window_size_for_expected_imbalance_estimation': 2000,\n",
       "  'initial_estimate_of_expected_n_ticks_in_bar': 50},\n",
       " {'ticker': 'BTCUSDT',\n",
       "  'time_frame': '1m',\n",
       "  'Sampling_Scheme': 'expected_tick_run_bars',\n",
       "  'n_trials': 50,\n",
       "  'n_samples': 39167,\n",
       "  'batch_size': 30275161,\n",
       "  'Shapiro-Wilk_statistic': 0,\n",
       "  'Shapiro-Wilk_pvalue': 0,\n",
       "  'window_size_for_expected_n_ticks_estimation': 5,\n",
       "  'window_size_for_expected_imbalance_estimation': 2000,\n",
       "  'initial_estimate_of_expected_n_ticks_in_bar': 50},\n",
       " {'ticker': 'BTCUSDT',\n",
       "  'time_frame': '1m',\n",
       "  'Sampling_Scheme': 'expected_volume_run_bars',\n",
       "  'n_trials': 50,\n",
       "  'n_samples': 39167,\n",
       "  'batch_size': 30275161,\n",
       "  'Shapiro-Wilk_statistic': 0,\n",
       "  'Shapiro-Wilk_pvalue': 0,\n",
       "  'window_size_for_expected_n_ticks_estimation': 5,\n",
       "  'window_size_for_expected_imbalance_estimation': 2000,\n",
       "  'initial_estimate_of_expected_n_ticks_in_bar': 50},\n",
       " {'ticker': 'BTCUSDT',\n",
       "  'time_frame': '1m',\n",
       "  'Sampling_Scheme': 'expected_dollar_run_bars',\n",
       "  'n_trials': 50,\n",
       "  'n_samples': 39167,\n",
       "  'batch_size': 30275161,\n",
       "  'Shapiro-Wilk_statistic': 0,\n",
       "  'Shapiro-Wilk_pvalue': 0,\n",
       "  'window_size_for_expected_n_ticks_estimation': 5,\n",
       "  'window_size_for_expected_imbalance_estimation': 2000,\n",
       "  'initial_estimate_of_expected_n_ticks_in_bar': 50}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_db_filename = 'params_resampling-scheme.json'\n",
    "# Open and load JSON file\n",
    "\n",
    "def load_param_resampling_db(file_name):\n",
    "    \"\"\"\n",
    "    Load resampling parameter database from a JSON file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_name : str\n",
    "        Path to the JSON file containing the parameter database.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "        A list of parameter dictionaries previously stored.\n",
    "        If the file does not exist or is invalid, returns an empty list.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The expected structure is a list of dictionaries where each dictionary\n",
    "      contains keys such as 'Sampling_Scheme', 'n_trials', 'n_samples', etc.\n",
    "    - Use together with `sampling_best_params` for managing optimal bar parameters.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> params_db = load_param_resampling_db(\"params_resampling-scheme.json\")\n",
    "    >>> len(params_db)\n",
    "    5\n",
    "    \"\"\"\n",
    "    with open(file_name, \"r\") as f:\n",
    "        df_params = json.load(f)\n",
    "    return df_params\n",
    "\n",
    "db_resampling_params = load_param_resampling_db(param_db_filename)\n",
    "db_resampling_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_param = pd.DataFrame(db_resampling_params)\n",
    "\n",
    "# df_param.to_csv(\"params_resampling-scheme.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load CSV\n",
    "# csv_file = \"params_resampling-scheme.csv\"\n",
    "# df = pd.read_csv(csv_file)\n",
    "\n",
    "# # Convert to JSON\n",
    "# json_file = csv_file.replace(\".csv\", \".json\")\n",
    "# df.to_json(json_file, orient=\"records\", indent=4)\n",
    "\n",
    "# print(f\"Saved JSON to {json_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load best parameters in the resampling database\n",
    "def db_resampling_best_params(_sampling_method,db_params):\n",
    "    \"\"\"\n",
    "    Retrieve the best parameter set for a given resampling method\n",
    "    from the parameter database.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    _sampling_method : str\n",
    "        The sampling scheme to filter on, e.g.:\n",
    "        - \"expected_tick_imbalance_bars\"\n",
    "        - \"expected_volume_run_bars\"\n",
    "    db_params : list of dict\n",
    "        The resampling parameter database, typically loaded from JSON.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The best parameter dictionary for the given sampling scheme.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - Ranking is based on three criteria:\n",
    "        1. `n_samples` (ascending) ‚Üí prefer larger sample size.\n",
    "        2. `Shapiro-Wilk_pvalue` (ascending).\n",
    "        3. `Shapiro-Wilk_statistic` (ascending).\n",
    "    - The last row after sorting is returned (`tail(1)`), so effectively it picks \n",
    "      the parameters with *highest* sample size, *highest* p-value, and *highest* statistic.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> best = db_resampling_best_params(\"expected_tick_imbalance_bars\", params_db)\n",
    "    >>> best[\"n_samples\"]\n",
    "    25000\n",
    "    \"\"\"\n",
    "    db = pd.DataFrame(db_params)\n",
    "    db = db.loc[db['Sampling_Scheme'] ==_sampling_method]\n",
    "\n",
    "    # Sort by 'n_samples' first, then by 'Shapiro-Wilk_pvalue', then by 'Shapiro-Wilk_statistic'\n",
    "    db = db.sort_values(\n",
    "        by=['n_samples', 'Shapiro-Wilk_pvalue', 'Shapiro-Wilk_statistic'],\n",
    "        ascending=[True, True, True])   # example: n_samples ‚Üë, pvalue ‚Üì, stat ‚Üì\n",
    "\n",
    "    return db.tail(1).to_dict(orient=\"records\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ticker': 'BTCUSDT',\n",
       " 'time_frame': '1m',\n",
       " 'Sampling_Scheme': 'expected_dollar_imbalance_bars',\n",
       " 'n_trials': 50,\n",
       " 'n_samples': 39167,\n",
       " 'batch_size': 30275161,\n",
       " 'Shapiro-Wilk_statistic': 0,\n",
       " 'Shapiro-Wilk_pvalue': 0,\n",
       " 'window_size_for_expected_n_ticks_estimation': 5,\n",
       " 'window_size_for_expected_imbalance_estimation': 2000,\n",
       " 'initial_estimate_of_expected_n_ticks_in_bar': 50}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_best_params = db_resampling_best_params(_sampling_method='expected_dollar_imbalance_bars',db_params=db_resampling_params)\n",
    "db_best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilily Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summed (additive counts / volumes)\n",
    "\n",
    "We sum over the ticks inside each bar:\n",
    "\t‚Ä¢\tohlc_trades\n",
    "\t‚Ä¢\tohlc_taker_base\n",
    "\t‚Ä¢\tohlc_taker_quote\n",
    "\t‚Ä¢\ttr_volume_base\n",
    "\t‚Ä¢\ttr_volume_quote\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Last (state-like snapshots, take the latest tick in the bar)\n",
    "\n",
    "We forward-fill within the bar only, then take the last available value:\n",
    "\t‚Ä¢\tl1_bid, l1_ask, l1_mid, l1_spread\n",
    "\t‚Ä¢\tl1_bid_qty, l1_ask_qty, l1_imbalance\n",
    "\t‚Ä¢\tl2_bid_depth, l2_ask_depth, l2_depth_asymmetry\n",
    "\t‚Ä¢\tl2_bid_vwap, l2_ask_vwap, l2_bid_slope, l2_ask_slope\n",
    "\t‚Ä¢\tspot_price, perp_mark_price\n",
    "\t‚Ä¢\tbasis_abs, basis_pct\n",
    "\t‚Ä¢\tfunding_rate, next_funding_time_ms\n",
    "\n",
    "(These are ‚Äúlevels‚Äù or ‚Äústate variables‚Äù you‚Äôd want at the bar close.)\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Mean (averaged inside the bar)\n",
    "\t‚Ä¢\ttr_buy_sell_imbalance\n",
    "\n",
    "‚∏ª\n",
    "\n",
    "Weighted mean (value √ó volume / total volume)\n",
    "\t‚Ä¢\ttr_vwap (weighted by tr_volume_base)\n",
    "\n",
    "(This is the standard definition of VWAP: average trade price weighted by base-asset volume.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute log returns from a price series.l\n",
    "log_return = lambda s: np.log(s).diff().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Optimization with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization_Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _batch_size(df_raw):\n",
    "    # Load a small sample of your data\n",
    "    df_batch = df_raw.head(10000)  # or CSV, etc.\n",
    "\n",
    "    # Estimate memory usage per row\n",
    "    bytes_per_row = df_batch.memory_usage(deep=True).sum() / len(df_batch)\n",
    "\n",
    "    # Get available RAM (bytes)\n",
    "    avail_ram = psutil.virtual_memory().available\n",
    "\n",
    "    # Budget: use only 30% of available RAM to be safe\n",
    "    ram_budget = 0.3 * avail_ram\n",
    "\n",
    "    # Rough batch size estimate\n",
    "    batch_size = int(ram_budget / bytes_per_row)\n",
    "\n",
    "    print(f\"Bytes per row ‚âà {bytes_per_row:.1f}\")\n",
    "    print(f\"Available RAM ‚âà {avail_ram/1e9:.1f} GB\")\n",
    "    print(f\"Safe batch_size ‚âà {batch_size:,} rows\")\n",
    "\n",
    "    return batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_seach_space_parameter(input_df: pd.DataFrame) -> dict:\n",
    "    \"\"\"\n",
    "    Classify irregular DatetimeIndex into timeframe bucket & trading style\n",
    "    by using the median difference between consecutive timestamps.\n",
    "    \"\"\"\n",
    "    if not isinstance(input_df.index, pd.DatetimeIndex):\n",
    "        raise ValueError(\"DataFrame index must be a DatetimeIndex\")\n",
    "    \n",
    "    # Compute median difference in seconds\n",
    "    diffs = input_df.index.to_series().diff().dropna().dt.total_seconds()\n",
    "    median_sec = diffs.median()\n",
    "    \n",
    "    # Convert to minutes\n",
    "    minutes = median_sec / 60.0\n",
    "    \n",
    "    # Grid search for different trading styles\n",
    "\n",
    "    scalper = {\n",
    "        \"wn\":   [5, 10, 15],                 # window_size_for_expected_n_ticks_estimation\n",
    "        \"wi\":   [1_000, 2_500, 5_000],     # window_size_for_expected_imbalance_estimation\n",
    "        \"seed\": [50, 150],      # initial_estimate_of_expected_n_ticks_in_bar\n",
    "    }\n",
    "\n",
    "    day_trader = {\n",
    "        \"wn\":   [10, 20, 30],\n",
    "        \"wi\":   [5_000, 10_000, 20_000],\n",
    "        \"seed\": [2_000, 10_000, 20_000],\n",
    "    }\n",
    "\n",
    "    swing_trader = {\n",
    "        \"wn\":   [30, 50, 80],\n",
    "        \"wi\":   [15_000, 30_000, 60_000],\n",
    "        \"seed\": [10_000, 30_000, 60_000],\n",
    "    }\n",
    "\n",
    "    position_trader = {\n",
    "        \"wn\":   [50, 80, 100],\n",
    "        \"wi\":   [30_000, 80_000, 120_000],\n",
    "        \"seed\": [20_000, 80_000, 120_000],\n",
    "    }\n",
    "\n",
    "    # Classification\n",
    "    if minutes <= 5:\n",
    "        bucket = f\"{round(minutes)}m\"\n",
    "        style = \"Scalper\"\n",
    "        search_space = scalper\n",
    "    elif 10 <= minutes <= 60:\n",
    "        bucket = f\"{round(minutes)}m‚Äì1h\"\n",
    "        style = \"Day trader\"\n",
    "        search_space = day_trader \n",
    "    elif 240 <= minutes <= 1440:\n",
    "        bucket = f\"{round(minutes/60)}h‚Äì1d\"\n",
    "        style = \"Swing trader\"\n",
    "        search_space = swing_trader\n",
    "    elif minutes >= 10080:  # 7 days\n",
    "        bucket = \"1w+\"\n",
    "        style = \"Position trader\"\n",
    "        search_space =  position_trader\n",
    "    else:\n",
    "        bucket = f\"{minutes:.1f}m\"\n",
    "        style = \"Unclassified\"\n",
    "    \n",
    "    return {\n",
    "        \"median_interval_min\": minutes,\n",
    "        \"bucket\": bucket,\n",
    "        \"style\": style,\n",
    "        \"grid_search\": search_space} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pyswarms\n",
    "import json\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pyswarms as ps  # <-- PSO\n",
    "# keep your existing imports for controller, db_resampling_best_params, etc.\n",
    "\n",
    "\n",
    "# ---------- retention penalty ----------\n",
    "def penalty(raw_df, sampling_output):\n",
    "    \"\"\"\n",
    "    Penalize retention outside [0.15, 0.50].\n",
    "    Returns (penalty, retention_ratio).\n",
    "    \"\"\"\n",
    "    retention_ratio = len(sampling_output) / max(1, len(raw_df))\n",
    "    if retention_ratio < 0.15:\n",
    "        pen = (0.15 - retention_ratio) * 10\n",
    "    elif retention_ratio > 0.50:\n",
    "        pen = (retention_ratio - 0.50) * 10\n",
    "    else:\n",
    "        pen = 0.0\n",
    "    return pen, retention_ratio\n",
    "\n",
    "\n",
    "def sampling_best_params(\n",
    "    sampling_method,\n",
    "    df_sample,\n",
    "    params_resampling_db,\n",
    "    db_file_name,\n",
    "    chunk_size,\n",
    "    trials=50,              # PSO iters\n",
    "    time_frame_label=\"1m\",\n",
    "    n_particles=24,         # PSO swarm size\n",
    "    seed=123,\n",
    "):\n",
    "    \"\"\"\n",
    "    Tune hyperparameters for imbalance/run bar sampling using\n",
    "    Jarque‚ÄìBera (JB) + retention penalty with Particle Swarm Optimization (PySwarms).\n",
    "    \"\"\"\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(df_sample)\n",
    "\n",
    "    # --------- search ranges (same as your latest) ----------\n",
    "    wn_range   = (1.5, 2_000)                            # window_size_for_expected_n_ticks_estimation\n",
    "    wi_range   = (100, min(20_000, max(500, n // 10))) # window_size_for_expected_imbalance_estimation\n",
    "    seed_range = (20, 1_000)                           # initial_estimate_of_expected_n_ticks_in_bar\n",
    "\n",
    "    # We'll optimize in log10 space for all three, then exponentiate and round.\n",
    "    lb = np.log10([wn_range[0],  wi_range[0],  seed_range[0]]).astype(float)\n",
    "    ub = np.log10([wn_range[1],  wi_range[1],  seed_range[1]]).astype(float)\n",
    "\n",
    "    # --------- preload existing best from DB (if any) ----------\n",
    "    try:\n",
    "        _db_best_params = db_resampling_best_params(\n",
    "            _sampling_method=sampling_method,\n",
    "            db_params=params_resampling_db,\n",
    "        )\n",
    "    except Exception:\n",
    "        _db_best_params = None\n",
    "\n",
    "    # --------- persist helper ----------\n",
    "    def _save_to_db(_new_params: dict):\n",
    "        params_resampling_db.append(_new_params)\n",
    "        with open(db_file_name, \"w\") as f:\n",
    "            json.dump(params_resampling_db, f, indent=4)\n",
    "\n",
    "    # --------- selection helper ----------\n",
    "    def _output(_old_param, _new_param, param_db, sample_size):\n",
    "        if _old_param is None:\n",
    "            _save_to_db(_new_param)\n",
    "            return _new_param\n",
    "        if _new_param in param_db:\n",
    "            print(\"‚ö†Ô∏è Already exists.\")\n",
    "            return _old_param\n",
    "        if _new_param.get(\"JB_pvalue\") is None:\n",
    "            print(\"‚ÑπÔ∏è Invalid trial, keeping existing best.\")\n",
    "            return _old_param\n",
    "        if sample_size > _old_param.get(\"n_samples\", 0):\n",
    "            _save_to_db(_new_param)\n",
    "            print(\"‚úÖ Larger sample size, updated.\")\n",
    "            return _new_param\n",
    "        if (sample_size == _old_param.get(\"n_samples\", 0)\n",
    "            and _new_param[\"JB_pvalue\"] > _old_param.get(\"JB_pvalue\", -1)):\n",
    "            _save_to_db(_new_param)\n",
    "            print(\"üîÑ Same sample size, better JB p-value, updated.\")\n",
    "            return _new_param\n",
    "        print(\"‚ÑπÔ∏è Did not satisfy update conditions ‚Äî keeping existing best.\")\n",
    "        return _old_param\n",
    "\n",
    "    # --------- PSO objective over the swarm ----------\n",
    "    # x has shape (n_particles, 3) in log10-space\n",
    "    BIG = 1e12  # big penalty for infeasible configs\n",
    "\n",
    "    def _eval_single(log_params):\n",
    "        # transform from log10 to int hyperparams (clip to bounds)\n",
    "        wn  = int(np.clip(round(10 ** log_params[0]), wn_range[0], wn_range[1]))\n",
    "        wi  = int(np.clip(round(10 ** log_params[1]), wi_range[0], wi_range[1]))\n",
    "        seed_ticks = int(np.clip(round(10 ** log_params[2]), seed_range[0], seed_range[1]))\n",
    "\n",
    "        # feasibility constraints (soft penalties):\n",
    "        # give headroom to estimate imbalance vs tpb window, and avoid mega seeds\n",
    "        if wi < 2 * wn:\n",
    "            return BIG\n",
    "        if seed_ticks > 5 * wn:\n",
    "            return BIG\n",
    "\n",
    "        try:\n",
    "            bars = controller.handle_input_command(\n",
    "                method_name=sampling_method,\n",
    "                method_arguments={\n",
    "                    \"window_size_for_expected_n_ticks_estimation\": wn,\n",
    "                    \"window_size_for_expected_imbalance_estimation\": wi,\n",
    "                    \"initial_estimate_of_expected_n_ticks_in_bar\": seed_ticks,\n",
    "                },\n",
    "                input_data=df_sample,\n",
    "                batch_size=chunk_size,\n",
    "            )\n",
    "        except Exception:\n",
    "            return BIG  # failed build ‚Üí penalize\n",
    "\n",
    "        # Minimum viable number of bars\n",
    "        min_bars = max(200, n // 200)\n",
    "        if len(bars) < min_bars:\n",
    "            return BIG\n",
    "\n",
    "        # Returns\n",
    "        close_col = \"Close\" if \"Close\" in bars.columns else bars.columns[-1]\n",
    "        rets = np.diff(np.log(bars[close_col].astype(float).to_numpy()))\n",
    "        rets = rets[np.isfinite(rets)]\n",
    "        if rets.size < 3 or np.var(rets) <= 1e-12:\n",
    "            return BIG\n",
    "\n",
    "        # JB + retention penalty\n",
    "        jb_stat, _ = stats.jarque_bera(rets)\n",
    "        ret_pen, _ = penalty(df_sample, bars)\n",
    "\n",
    "        return float(jb_stat) + float(ret_pen)\n",
    "\n",
    "    def pso_objective(X):\n",
    "        # X: (n_particles, dim)\n",
    "        return np.array([_eval_single(row) for row in X], dtype=float)\n",
    "\n",
    "    # --------- set up initial swarm positions (optional warm start) ----------\n",
    "    # default random in bounds:\n",
    "    init_pos = rng.uniform(low=lb, high=ub, size=(n_particles, 3))\n",
    "\n",
    "    # if DB best exists, seed first particle near it\n",
    "    if _db_best_params is not None:\n",
    "        try:\n",
    "            seed_vec = np.log10([\n",
    "                np.clip(int(_db_best_params[\"window_size_for_expected_n_ticks_estimation\"]), *wn_range),\n",
    "                np.clip(int(_db_best_params[\"window_size_for_expected_imbalance_estimation\"]), *wi_range),\n",
    "                np.clip(int(_db_best_params[\"initial_estimate_of_expected_n_ticks_in_bar\"]), *seed_range),\n",
    "            ])\n",
    "            init_pos[0] = np.clip(seed_vec, lb, ub)\n",
    "        except Exception:\n",
    "            pass\n",
    "    else:\n",
    "        # seed center as a reasonable mid guess\n",
    "        mid_vec = np.log10([\n",
    "            (wn_range[0] + wn_range[1]) / 2,\n",
    "            (wi_range[0] + wi_range[1]) / 2,\n",
    "            (seed_range[0] + seed_range[1]) / 2,\n",
    "        ])\n",
    "        init_pos[0] = np.clip(mid_vec, lb, ub)\n",
    "\n",
    "    # --------- run PSO ----------\n",
    "    bounds = (lb, ub)\n",
    "    options = dict(c1=0.5, c2=0.3, w=0.9)  # cognitive, social, inertia\n",
    "\n",
    "    optimizer = ps.single.GlobalBestPSO(\n",
    "        n_particles=n_particles,\n",
    "        dimensions=3,\n",
    "        options=options,\n",
    "        bounds=bounds,\n",
    "        init_pos=init_pos,\n",
    "        ftol=1e-8,        # early stop tolerance on cost improvement\n",
    "        ftol_iter=10,     # patience (iters)\n",
    "    )\n",
    "\n",
    "    best_cost, best_pos = optimizer.optimize(pso_objective, iters=trials, verbose=True)\n",
    "\n",
    "    # --------- evaluate best solution to collect metrics ----------\n",
    "    def _transform_and_eval(log_params):\n",
    "        wn  = int(np.clip(round(10 ** log_params[0]), wn_range[0], wn_range[1]))\n",
    "        wi  = int(np.clip(round(10 ** log_params[1]), wi_range[0], wi_range[1]))\n",
    "        seed_ticks = int(np.clip(round(10 ** log_params[2]), seed_range[0], seed_range[1]))\n",
    "\n",
    "        bars = controller.handle_input_command(\n",
    "            method_name=sampling_method,\n",
    "            method_arguments={\n",
    "                \"window_size_for_expected_n_ticks_estimation\": wn,\n",
    "                \"window_size_for_expected_imbalance_estimation\": wi,\n",
    "                \"initial_estimate_of_expected_n_ticks_in_bar\": seed_ticks,\n",
    "            },\n",
    "            input_data=df_sample,\n",
    "            batch_size=chunk_size,\n",
    "        )\n",
    "\n",
    "        # metrics\n",
    "        close_col = \"Close\" if \"Close\" in bars.columns else bars.columns[-1]\n",
    "        rets = np.diff(np.log(bars[close_col].astype(float).to_numpy()))\n",
    "        rets = rets[np.isfinite(rets)]\n",
    "        jb_stat, jb_pval = stats.jarque_bera(rets)\n",
    "        ret_pen, ratio = penalty(df_sample, bars)\n",
    "        return dict(wn=wn, wi=wi, seed_ticks=seed_ticks,\n",
    "                    jb_stat=float(jb_stat), jb_pval=float(jb_pval),\n",
    "                    retention_ratio=float(ratio),\n",
    "                    loss=float(jb_stat) + float(ret_pen))\n",
    "\n",
    "    best_metrics = _transform_and_eval(best_pos)\n",
    "    sample_size = len(df_sample)\n",
    "\n",
    "    new_params = {\n",
    "        \"ticker\": df_sample.index.unique()[0] if len(df_sample.index) else \"UNKNOWN\",\n",
    "        \"time_frame\": time_frame_label,\n",
    "        \"Sampling_Scheme\": sampling_method,\n",
    "        \"n_trials\": trials,\n",
    "        \"n_samples\": sample_size,\n",
    "        \"batch_size\": chunk_size,\n",
    "        \"JB_statistic\": best_metrics[\"jb_stat\"],\n",
    "        \"JB_pvalue\": best_metrics[\"jb_pval\"],\n",
    "        \"retention_ratio\": best_metrics[\"retention_ratio\"],\n",
    "        \"window_size_for_expected_n_ticks_estimation\": best_metrics[\"wn\"],\n",
    "        \"window_size_for_expected_imbalance_estimation\": best_metrics[\"wi\"],\n",
    "        \"initial_estimate_of_expected_n_ticks_in_bar\": best_metrics[\"seed_ticks\"],\n",
    "    }\n",
    "\n",
    "    # --------- decide whether to persist ----------\n",
    "    chosen = _output(\n",
    "        _old_param=_db_best_params,\n",
    "        _new_param=new_params,\n",
    "        param_db=params_resampling_db,\n",
    "        sample_size=sample_size,\n",
    "    )\n",
    "    return chosen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Sampling Schemes Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Imbalance Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bytes per row ‚âà 165.0\n",
      "Available RAM ‚âà 16.2 GB\n",
      "Safe batch_size ‚âà 29,433,260 rows\n"
     ]
    }
   ],
   "source": [
    "max_batch_size = _batch_size(df_raw=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 100\n",
    "n_pat = 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imbalance Tick Bars Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-15 20:10:33,792 - pyswarms.single.global_best - INFO - Optimize for 100 iters with {'c1': 0.5, 'c2': 0.3, 'w': 0.9}\n",
      "pyswarms.single.global_best:   0%|          |0/100"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:   1%|          |1/100, best_cost=8.42e+6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:   2%|‚ñè         |2/100, best_cost=8.4e+6 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:   3%|‚ñé         |3/100, best_cost=8.4e+6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:   4%|‚ñç         |4/100, best_cost=8.4e+6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:   5%|‚ñå         |5/100, best_cost=8.4e+6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:   6%|‚ñå         |6/100, best_cost=8.4e+6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:   7%|‚ñã         |7/100, best_cost=8.4e+6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:   8%|‚ñä         |8/100, best_cost=8.4e+6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:   9%|‚ñâ         |9/100, best_cost=8.4e+6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pyswarms.single.global_best:  10%|‚ñà         |10/100, best_cost=8.4e+6"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n",
      "Processing batch 0 with size 39167\n"
     ]
    }
   ],
   "source": [
    "params_expected_tick_bars = sampling_best_params(sampling_method='expected_tick_imbalance_bars', \n",
    "                                                df_sample=df_riskAI, \n",
    "                                                params_resampling_db=db_resampling_params, \n",
    "                                                db_file_name=param_db_filename ,                                                \n",
    "                                                chunk_size = max_batch_size , \n",
    "                                                trials=n_trials, \n",
    "                                                n_particles=n_pat,\n",
    "                                                time_frame_label=\"1m\"\n",
    ")\n",
    "\n",
    "params_expected_tick_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imbalance Volume Bars Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_expected_volume_bars = sampling_best_params(sampling_method='expected_volume_imbalance_bars', \n",
    "                                                df_sample=df_riskAI, \n",
    "                                                params_resampling_db=db_resampling_params, \n",
    "                                                db_file_name=param_db_filename,\n",
    "                                                chunk_size = max_batch_size , \n",
    "                                                trials=n_trials, \n",
    "                                                n_particles=n_pat,\n",
    "                                                time_frame_label=\"1m\"\n",
    ")\n",
    "\n",
    "params_expected_volume_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imbalance Dollar Bars Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_expected_dollar_bars = sampling_best_params(sampling_method='expected_dollar_imbalance_bars', \n",
    "                                                df_sample=df_riskAI, \n",
    "                                                params_resampling_db=db_resampling_params, \n",
    "                                                db_file_name=param_db_filename ,\n",
    "                                                chunk_size = max_batch_size , \n",
    "                                                trials=n_trials, \n",
    "                                                n_particles=n_pat,\n",
    "                                                time_frame_label=\"1m\"\n",
    ")\n",
    "\n",
    "params_expected_dollar_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Run Bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Bars - Tick Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_run_tick_bars = sampling_best_params(sampling_method='expected_tick_run_bars', \n",
    "                                                df_sample=df_riskAI, \n",
    "                                                params_resampling_db=db_resampling_params, \n",
    "                                                db_file_name=param_db_filename,\n",
    "                                                chunk_size = max_batch_size , \n",
    "                                                trials=n_trials, \n",
    "                                                n_particles=n_pat,\n",
    "                                                time_frame_label=\"1m\"\n",
    ")\n",
    "\n",
    "params_run_tick_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_run_volume_bars = sampling_best_params(sampling_method='expected_volume_run_bars', \n",
    "                                                df_sample=df_riskAI, \n",
    "                                                params_resampling_db=db_resampling_params, \n",
    "                                                db_file_name=param_db_filename,\n",
    "                                                chunk_size = max_batch_size , \n",
    "                                                trials=n_trials, \n",
    "                                                n_particles=n_pat,\n",
    "                                                time_frame_label=\"1m\"\n",
    ")\n",
    "\n",
    "params_run_volume_bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_run_dollar_bars = sampling_best_params(sampling_method='expected_dollar_run_bars', \n",
    "                                                df_sample=df_riskAI, \n",
    "                                                params_resampling_db=db_resampling_params, \n",
    "                                                db_file_name=param_db_filename,\n",
    "                                                chunk_size = max_batch_size , \n",
    "                                                trials=n_trials, \n",
    "                                                n_particles=n_pat,\n",
    "                                                time_frame_label=\"1m\"\n",
    ")\n",
    "\n",
    "params_run_dollar_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload Resampling Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_db_resampling_params = load_param_resampling_db(param_db_filename)\n",
    "reloaded_db_resampling_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Bars - Volume Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run Bars - Dollar Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling schemes\n",
    "\n",
    "In financial time series, sampling schemes determine how raw tick-level data (individual trades) are aggregated into bars (OHLC structures). Traditional time bars sample at fixed calendar intervals, but these often distort statistical properties by oversampling quiet periods and undersampling volatile ones.\n",
    "\n",
    "To address this, L√≥pez de Prado (2018) introduced alternative, `event-driven` bars that adapt to market activity. In this work, the focus is on:\n",
    "\n",
    "- `Expected Imbalance Bars (EIBs)`\n",
    "EIBs close a bar when the accumulated buy‚Äìsell volume imbalance exceeds an expected threshold, estimated dynamically from historical data. This produces bars of variable length that contain approximately equal amounts of information, improving stationarity and normality of returns. EIBs are particularly well suited for machine learning tasks that rely on balanced and stable input data.\n",
    "\n",
    "- `Expected Run Bars (ERBs)`\n",
    "ERBs close a bar when the number of consecutive buy or sell trades (a ‚Äúrun‚Äù) surpasses an expected run length, again estimated adaptively. This highlights periods of persistent order flow, often associated with informed trading or liquidity grabs. ERBs are especially valuable for detecting market microstructure patterns, such as those studied in Smart Money Concepts (SMC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_info_driven_bars(sampling_method,df_sample,db_best_params):\n",
    "\n",
    "    best_params = db_resampling_best_params(_sampling_method='expected_dollar_imbalance_bars',db_params=db_best_params)\n",
    "    info_driven_bar = controller.handle_input_command(\n",
    "    method_name=sampling_method,\n",
    "    method_arguments={\n",
    "        \"window_size_for_expected_n_ticks_estimation\": best_params['window_size_for_expected_n_ticks_estimation'],\n",
    "        \"window_size_for_expected_imbalance_estimation\": best_params['window_size_for_expected_imbalance_estimation'],\n",
    "        \"initial_estimate_of_expected_n_ticks_in_bar\": best_params['initial_estimate_of_expected_n_ticks_in_bar'],\n",
    "    },\n",
    "    input_data=df_sample,\n",
    "    batch_size=best_params['batch_size'],\n",
    "    )\n",
    "\n",
    "    return info_driven_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    'ohlc_trades','ohlc_taker_base','ohlc_taker_quote',\n",
    "    'l1_bid','l1_ask','l1_mid','l1_spread',\n",
    "    'l1_bid_qty','l1_ask_qty','l1_imbalance',\n",
    "    'l2_bid_depth','l2_ask_depth','l2_depth_asymmetry',\n",
    "    'l2_bid_vwap','l2_ask_vwap','l2_bid_slope','l2_ask_slope',\n",
    "    'tr_volume_base','tr_volume_quote','tr_vwap','tr_buy_sell_imbalance',\n",
    "    'spot_price','perp_mark_price','basis_abs','basis_pct',\n",
    "    'funding_rate','next_funding_time_ms'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expected Imbalance Bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalance Tick Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIB_ticks = generate_info_driven_bars(sampling_method='expected_tick_imbalance_bars',\n",
    "                                      df_sample=df_riskAI,\n",
    "                                      db_best_params=reloaded_db_resampling_params)\n",
    "\n",
    "EIB_ticks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalance Volume Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIB_vol = generate_info_driven_bars(sampling_method='expected_volume_imbalance_bars',\n",
    "                                      df_sample=df_riskAI,\n",
    "                                      db_best_params=reloaded_db_resampling_params)\n",
    "EIB_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalance Dollar Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EIB_dollar = generate_info_driven_bars(sampling_method='expected_dollar_imbalance_bars',\n",
    "                                      df_sample=df_riskAI,\n",
    "                                      db_best_params=reloaded_db_resampling_params)\n",
    "EIB_dollar\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_returns = log_return(df_riskAI['price'])\n",
    "ticks_EIB_returns = log_return(EIB_ticks['Close'])\n",
    "volume_EIB_returns = log_return(EIB_vol['Close'])\n",
    "dollars_EIB_returns = log_return(EIB_dollar['Close'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jb_report(name, s):\n",
    "    # ensure 1-D array/Series\n",
    "    s = pd.Series(s).astype(float)\n",
    "    # drop non-finite\n",
    "    s = s.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "    n = len(s)\n",
    "    if n < 3:\n",
    "        print(f\"{name}: not enough data for Jarque‚ÄìBera (n={n})\")\n",
    "        return\n",
    "\n",
    "    res = stats.jarque_bera(s)\n",
    "    print(f\"{name}: JB stat={res.statistic:.6g}, p={res.pvalue:.3g}, n={n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Jarque‚ÄìBera test statistic \n",
    "\n",
    "The `Jarque‚ÄìBera (JB) test` is used to check whether data follow a normal distribution by looking at skewness and kurtosis. In this test, smaller values are desirable because they indicate the data are closer to being normally distributed. For example, a statistic around 1 suggests the data are reasonably consistent with normality. A very large value, such as 6,633,374, strongly signals that the data deviate from normality, often due to heavy tails or asymmetry. In rare cases, the statistic can be 0, which occurs if the data have exactly zero skewness and a normal level of kurtosis, or if the dataset has no variation at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Jarque-Bera test statistic for time returns:\", int(stats.jarque_bera(time_returns)[0]))\n",
    "# print(\"Jarque-Bera test statistic for EIB tick returns:\", int(stats.jarque_bera(ticks_EIB_returns)[0]))\n",
    "# print(\"Jarque-Bera test statistic for EIB volume returns:\", int(stats.jarque_bera(volume_EIB_returns)[0]))\n",
    "# print(\"Jarque-Bera test statistic for EIB dollar returns:\", int(stats.jarque_bera(dollars_EIB_returns)[0]))\n",
    "\n",
    "jb_report(\"time returns\", time_returns)\n",
    "jb_report(\"EIB tick returns\", ticks_EIB_returns)\n",
    "jb_report(\"EIB volume returns\", volume_EIB_returns)\n",
    "jb_report(\"EIB dollar returns\", dollars_EIB_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapiro-Wilk Test\n",
    "\n",
    "The `Shapiro‚ÄìWilk` test is a statistical method used to check whether a dataset follows a normal distribution. Unlike the Jarque‚ÄìBera test, which looks at skewness and kurtosis, the Shapiro‚ÄìWilk test directly compares the data to a perfectly normal shape. The test produces a statistic between 0 and 1, where values closer to 1 indicate the data are more consistent with normality. For example, a statistic of 0.98 would suggest the data are likely normal, while a much smaller value, such as 0.70, would indicate a strong departure from normality. The test also provides a p-value: if it is larger than 0.05, the data are considered roughly normal; if smaller, the data are unlikely to be normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapiro_report(name, s):\n",
    "    s = pd.Series(s).astype(float)\n",
    "    s = s.replace([np.inf, -np.inf], np.nan).dropna()\n",
    "    n = len(s)\n",
    "\n",
    "    if n < 3:\n",
    "        print(f\"{name}: not enough data for Shapiro‚ÄìWilk (n={n})\")\n",
    "        return\n",
    "\n",
    "    # subsample if > 5000 (recommended by SciPy docs)\n",
    "    if n > 5000:\n",
    "        rng = np.random.default_rng(42)\n",
    "        s = rng.choice(s, 5000, replace=False)\n",
    "\n",
    "    stat, pval = stats.shapiro(s)\n",
    "    print(f\"{name}: W={stat:.6f}, p={pval:.3g}, n={n}\")\n",
    "\n",
    "# usage\n",
    "shapiro_report(\"time returns\", time_returns)\n",
    "shapiro_report(\"EIB tick returns\", ticks_EIB_returns)\n",
    "shapiro_report(\"EIB volume returns\", volume_EIB_returns)\n",
    "shapiro_report(\"EIB dollar returns\", dollars_EIB_returns)\n",
    "\n",
    "# print(\"Shapiro-Wilk test statistic for time returns:\", stats.shapiro(time_returns))\n",
    "# print(\"Shapiro-Wilk test statistic for EIB tick returns:\", stats.shapiro(ticks_EIB_returns))\n",
    "# print(\"Shapiro-Wilk test statistic for EIB volume returns:\", stats.shapiro(volume_EIB_returns))\n",
    "# print(\"Shapiro-Wilk test statistic for EIB dollar returns:\", stats.shapiro(dollars_EIB_returns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Density Estimate (KDE) plot\n",
    "\n",
    "A `Kernel Density Estimate (KDE) plot` is a smooth curve that shows the probability distribution of a dataset. It can be thought of as a smoothed version of a histogram, where the peaks indicate where the data are most concentrated and the shape of the curve shows how the values are distributed. KDE plots are often used to visually assess whether data resemble a normal distribution or display skewness, heavy tails, or multiple peaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize Data \n",
    "time_standard = (time_returns - time_returns.mean()) / time_returns.std()\n",
    "EIB_tick_standard = (ticks_EIB_returns - ticks_EIB_returns.mean()) / ticks_EIB_returns.std()\n",
    "EIB_volume_standard = (volume_EIB_returns  - volume_EIB_returns.mean()) / volume_EIB_returns.std()\n",
    "EIB_dollar_standard = (dollars_EIB_returns - dollars_EIB_returns.mean()) / dollars_EIB_returns.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution Plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.kdeplot(time_standard, label=\"Time\", color=\"red\")\n",
    "sns.kdeplot(EIB_tick_standard, label=\"Tick\", color=\"blue\")\n",
    "sns.kdeplot(EIB_volume_standard, label=\"Volume\", color=\"green\")\n",
    "sns.kdeplot(EIB_dollar_standard, label=\"Dollar\", color=\"purple\", linestyle=\"-.\")\n",
    "sns.kdeplot(np.random.normal(size=1000000), label=\"Normal\", linestyle=\"dotted\")\n",
    "plt.xticks(range(-4, +4))\n",
    "\n",
    "#labels\n",
    "plt.xlabel(\"Standardized Log Returns\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\n",
    "    'Partial Recovery of Normality for Expected Imbalance Bars',\n",
    "    loc='center', \n",
    ")\n",
    "plt.xlim(-5, 5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tick Run Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_run_bars = generate_info_driven_bars(sampling_method='expected_tick_run_bars',\n",
    "                                      df_sample=df_riskAI,\n",
    "                                      db_best_params=reloaded_db_resampling_params)\n",
    "tick_run_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Volume Run Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "volume_run_bars = generate_info_driven_bars(sampling_method='expected_volume_run_bars',\n",
    "                                      df_sample=df_riskAI,\n",
    "                                      db_best_params=reloaded_db_resampling_params)\n",
    "volume_run_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dollar Run Bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dollar_run_bars = generate_info_driven_bars(sampling_method='expected_dollar_run_bars',\n",
    "                                      df_sample=df_riskAI,\n",
    "                                      db_best_params=reloaded_db_resampling_params)\n",
    "dollar_run_bars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Log Return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick_run_bars_returns = log_return(tick_run_bars['Close'])\n",
    "volume_run_bars_returns = log_return(volume_run_bars['Close'])\n",
    "dollar_run_bars_returns = log_return(dollar_run_bars['Close'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Jarque‚ÄìBera test statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Jarque-Bera test statistic for time returns:\", int(stats.jarque_bera(time_returns)[0]))\n",
    "print(\"Jarque-Bera test statistic for tick run bars returns:\", int(stats.jarque_bera(tick_run_bars_returns)[0]))\n",
    "print(\"Jarque-Bera test statistic for volume run bars returns:\", int(stats.jarque_bera(volume_run_bars_returns)[0]))\n",
    "print(\"Jarque-Bera test statistic for dollar run bars returns:\", int(stats.jarque_bera(dollar_run_bars_returns)[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shapiro-Wilk Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shapiro-Wilk test statistic for time returns:\", stats.shapiro(time_returns))\n",
    "print(\"Shapiro-Wilk test statistic for tick run bars returns:\", stats.shapiro(tick_run_bars_returns))\n",
    "print(\"Shapiro-Wilk test statistic for volume run bars returns:\", stats.shapiro(volume_run_bars_returns))\n",
    "print(\"Shapiro-Wilk test statistic for dollar run bars returns:\", stats.shapiro(dollar_run_bars_returns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kernel Density Estimate (KDE) plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardize Data \n",
    "tick_run_bars_standard = (tick_run_bars_returns - tick_run_bars_returns.mean()) / tick_run_bars_returns.std()\n",
    "volume_run_bars_standard = (volume_run_bars_returns  - volume_run_bars_returns.mean()) / volume_run_bars_returns.std()\n",
    "dollar_run_bars_standard = (dollar_run_bars_returns - dollar_run_bars_returns.mean()) / dollar_run_bars_returns.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution Plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.kdeplot(time_standard, label=\"Time\", color=\"red\")\n",
    "sns.kdeplot(tick_run_bars_standard, label=\"Tick\", color=\"blue\")\n",
    "sns.kdeplot(volume_run_bars_standard, label=\"Volume\", color=\"green\")\n",
    "sns.kdeplot(dollar_run_bars_standard , label=\"Dollar\", color=\"purple\", linestyle=\"-.\")\n",
    "sns.kdeplot(np.random.normal(size=1000000), label=\"Normal\", linestyle=\"dotted\")\n",
    "plt.xticks(range(-4, +4))\n",
    "\n",
    "#labels\n",
    "plt.xlabel(\"Standardized Log Returns\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.title(\n",
    "    'Partial Recovery of Normality for Run Bars',\n",
    "    loc='center', \n",
    ")\n",
    "plt.xlim(-5, 5)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_features(raw_df, resumpling_df, feature_name, agg_func=np.sum):\n",
    "    \"\"\"\n",
    "    Aggregate a feature between Tick Number ranges using a specified aggregation function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_df : pd.DataFrame\n",
    "        Full DataFrame with tick-by-tick data.\n",
    "    resumpling_df : pd.DataFrame\n",
    "        DataFrame with \"Tick Number\" boundaries (breakpoints).\n",
    "    feature_name : str\n",
    "        Column name in raw_df to aggregate (e.g., \"ohlc_trades\").\n",
    "    agg_func : function, default=np.sum\n",
    "        Aggregation function (e.g., np.sum, np.mean, np.max).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        resumpling_df with the aggregated feature filled.\n",
    "    \"\"\"\n",
    "\n",
    "    resumpling_df[feature_name] = np.nan  \n",
    "\n",
    "    m = raw_df[feature_name]\n",
    "    idx_list = resumpling_df.index.astype(int).to_list()\n",
    "\n",
    "    if idx_list:\n",
    "        idx_start = idx_list[0]\n",
    "        idx_end = idx_list[0] + 1\n",
    "        resumpling_df.loc[idx_start, feature_name] = agg_func(m.iloc[0:idx_end])\n",
    "\n",
    "    for start, end in zip(idx_list, idx_list[1:]):\n",
    "        resumpling_df.loc[end, feature_name] = agg_func(m.iloc[start:(end+1)])\n",
    "\n",
    "    return resumpling_df\n",
    "\n",
    "def last_state(raw_df,resumpling_df, feature_name):\n",
    "    \"\"\"\n",
    "    Align the latest state of a feature from the raw dataframe\n",
    "    onto the resampled dataframe at matching indices.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_df : pandas.DataFrame\n",
    "        Original/raw dataset containing the feature of interest.\n",
    "    resampling_df : pandas.DataFrame\n",
    "        Resampled dataset whose index is aligned to raw_df.\n",
    "    feature_name : str\n",
    "        Column name (feature) to propagate from raw_df to resampling_df.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Updated resampling_df with a new column `feature_name`\n",
    "        containing the values from raw_df at matching indices. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    resumpling_df[feature_name] = np.nan  \n",
    "    idx_list = resumpling_df.index.to_list()\n",
    "    for i in idx_list:\n",
    "        resumpling_df.loc[i,feature_name] = raw_df[feature_name][i]\n",
    "    return resumpling_df\n",
    "\n",
    "def price_vwap(raw_df, resumpling_df, feature_name, vol_colum_name ='ohlc_volume'):\n",
    "    \"\"\"\n",
    "    Compute VWAP (Volume-Weighted Average Price) for each resampled bar.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw_df : pandas.DataFrame\n",
    "        Original dataframe with price and volume data.\n",
    "    resampling_df : pandas.DataFrame\n",
    "        Resampled dataframe that defines the bar boundaries (by index).\n",
    "    feature_name : str\n",
    "        Column name in raw_df containing prices to weight.\n",
    "    vol_column_name : str, default=\"ohlc_volume\"\n",
    "        Column name in raw_df containing volume weights.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Updated resampling_df with an extra column containing VWAP values\n",
    "        for each resampled interval.\n",
    "    \"\"\"\n",
    "    \n",
    "    resumpling_df[feature_name] = np.nan  \n",
    "\n",
    "    m = raw_df[feature_name]\n",
    "    vol = raw_df[vol_colum_name]\n",
    "\n",
    "    idx_list = resumpling_df.index.astype(int).to_list()\n",
    "\n",
    "    if idx_list:\n",
    "        idx_start = idx_list[0]\n",
    "        idx_end = idx_list[0] + 1\n",
    "\n",
    "        resumpling_df.loc[idx_start, feature_name] = np.average(m[0:idx_end], weights=vol[0:idx_end])\n",
    "\n",
    "    for start, end in zip(idx_list, idx_list[1:]):\n",
    "        resumpling_df.loc[end, feature_name] = np.average(m[start:(end+1)], weights=vol[start:(end+1)])\n",
    "\n",
    "    return resumpling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_features(input_df, sampling_df):\n",
    "\n",
    "    \"\"\"\n",
    "    Aggregate raw features into resampled bars using different strategies.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_df : pandas.DataFrame\n",
    "        Raw tick-level dataframe containing all features.\n",
    "    sampling_df : pandas.DataFrame\n",
    "        DataFrame with resampled bar boundaries.\n",
    "        Must contain a 'Tick Number' column to align with input_df.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        Updated sampling_df containing aggregated features.\n",
    "    \"\"\"\n",
    "\n",
    "    sampling_df =sampling_df.set_index(\"Tick Number\")\n",
    "\n",
    "    vol_sum = ['ohlc_trades','ohlc_taker_base','ohlc_taker_quote',\n",
    "            'tr_volume_base','tr_volume_quote']\n",
    "\n",
    "    last_states = ['l1_bid','l1_ask','l1_mid','l1_spread',\n",
    "                'l1_bid_qty','l1_ask_qty','l1_imbalance',\n",
    "                'l2_bid_depth','l2_ask_depth','l2_depth_asymmetry',\n",
    "                'l2_bid_vwap','l2_ask_vwap','l2_bid_slope','l2_ask_slope',\n",
    "                'spot_price','perp_mark_price','basis_abs','basis_pct',\n",
    "                'funding_rate','next_funding_time_ms']\n",
    "\n",
    "    mean_bar= ['tr_buy_sell_imbalance']\n",
    "\n",
    "    weighted_mean = ['tr_vwap']\n",
    "\n",
    "    #total\n",
    "    for i in vol_sum:\n",
    "        sampling_df  = grouped_features(raw_df=input_df, \n",
    "                                        resumpling_df=sampling_df , \n",
    "                                        feature_name=i, \n",
    "                                        agg_func=np.sum)\n",
    "    #last value\n",
    "    for j in last_states:\n",
    "        sampling_df  = last_state(raw_df=input_df,\n",
    "                                    resumpling_df=sampling_df ,\n",
    "                                    feature_name=j)\n",
    "    #average value   \n",
    "    for k in mean_bar:\n",
    "        sampling_df  = grouped_features(raw_df=input_df, \n",
    "                                        resumpling_df=sampling_df , \n",
    "                                        feature_name=k, \n",
    "                                        agg_func=np.mean)\n",
    "    #weighted average\n",
    "    for _ in weighted_mean:\n",
    "        sampling_df  = price_vwap(raw_df=input_df, \n",
    "                                    resumpling_df=sampling_df , \n",
    "                                    feature_name= _,\n",
    "                                    vol_colum_name ='ohlc_volume')\n",
    "        \n",
    "    return sampling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = aggregate_features(input_df=df, sampling_df=dollar_run_bars)\n",
    "df_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_smc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
